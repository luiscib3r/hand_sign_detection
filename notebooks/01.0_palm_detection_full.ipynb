{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV: 4.8.1\n",
      "CSV: 1.0\n",
      "Numpy: 1.24.3\n",
      "Pandas: 2.0.3\n",
      "TensorFlow: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from cv2.typing import MatLike\n",
    "\n",
    "from non_maximum_suppression import non_max_suppression_fast\n",
    "\n",
    "print(f'OpenCV: {cv2.__version__}')\n",
    "print(f'CSV: {csv.__version__}')\n",
    "print(f'Numpy: {np.__version__}')\n",
    "print(f'Pandas: {pd.__version__}')\n",
    "print(f'TensorFlow: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 'Hand Tracking'\n",
    "PALM_MODEL_PATH = '../models/palm_detection_full.tflite'\n",
    "ANCHORS_PATH = '../MediaPipeSSDAnchors/anchors.csv'\n",
    "DETECTION_THRESHOLD = 0.5\n",
    "BOX_ENLARGE_FACTOR = 1.5\n",
    "BOX_SHIFT_FACTOR = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchors shape: (2016, 4)\n",
      "Anchors: [[0.02083333 0.02083333 1.         1.        ]\n",
      " [0.02083333 0.02083333 1.         1.        ]\n",
      " [0.0625     0.02083333 1.         1.        ]\n",
      " [0.0625     0.02083333 1.         1.        ]\n",
      " [0.10416667 0.02083333 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "csv_file = open(ANCHORS_PATH, 'r')\n",
    "anchors = np.r_[\n",
    "  [x for x in csv.reader(csv_file, quoting=csv.QUOTE_NONNUMERIC)]\n",
    "]\n",
    "\n",
    "print(f'Anchors shape: {anchors.shape}')\n",
    "print(f'Anchors: {anchors[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=PALM_MODEL_PATH)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                         0\n",
      "name                                                               input_1\n",
      "index                                                                    0\n",
      "shape                                                     [1, 192, 192, 3]\n",
      "shape_signature                                           [1, 192, 192, 3]\n",
      "dtype                                              <class 'numpy.float32'>\n",
      "quantization                                                      (0.0, 0)\n",
      "quantization_parameters  {'scales': [], 'zero_points': [], 'quantized_d...\n",
      "sparsity_parameters                                                     {}\n"
     ]
    }
   ],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "\n",
    "for detail in input_details:\n",
    "    print(pd.DataFrame.from_dict(detail, orient='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image input size: (192, 192)\n"
     ]
    }
   ],
   "source": [
    "IMG_INPUT_SIZE = tuple(input_details[0]['shape'][1:3])\n",
    "\n",
    "print(f'Image input size: {IMG_INPUT_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                         0\n",
      "name                                                              Identity\n",
      "index                                                                  279\n",
      "shape                                                        [1, 2016, 18]\n",
      "shape_signature                                              [1, 2016, 18]\n",
      "dtype                                              <class 'numpy.float32'>\n",
      "quantization                                                      (0.0, 0)\n",
      "quantization_parameters  {'scales': [], 'zero_points': [], 'quantized_d...\n",
      "sparsity_parameters                                                     {}\n",
      "                                                                         0\n",
      "name                                                            Identity_1\n",
      "index                                                                  276\n",
      "shape                                                         [1, 2016, 1]\n",
      "shape_signature                                               [1, 2016, 1]\n",
      "dtype                                              <class 'numpy.float32'>\n",
      "quantization                                                      (0.0, 0)\n",
      "quantization_parameters  {'scales': [], 'zero_points': [], 'quantized_d...\n",
      "sparsity_parameters                                                     {}\n"
     ]
    }
   ],
   "source": [
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "for detail in output_details:\n",
    "    print(pd.DataFrame.from_dict(detail, orient='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(frame: MatLike):\n",
    "    # Original Size\n",
    "    shape = np.r_[frame.shape]\n",
    "\n",
    "    # Calculate Padding\n",
    "    pad = (shape.max() - shape[:2]).astype('uint32') // 2\n",
    "\n",
    "    # Pad Image\n",
    "    img_pad = np.pad(frame, ((pad[0], pad[0]), (pad[1], pad[1]), (0, 0)), 'constant')\n",
    "\n",
    "    # Resize Image\n",
    "    img_small = cv2.resize(img_pad, IMG_INPUT_SIZE)\n",
    "    \n",
    "    # Set as contiguous array (speed up)\n",
    "    img_small = np.ascontiguousarray(img_small)\n",
    "\n",
    "    img_norm = (img_small / 255).astype('float32')\n",
    "    \n",
    "    return img_pad, img_norm, pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(img_norm):\n",
    "    # Add Batch dimension\n",
    "    img_input = np.expand_dims(img_norm, axis=0)\n",
    "\n",
    "    # Set input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], img_input)\n",
    "\n",
    "    # Run Inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get Output\n",
    "    \"\"\"\n",
    "    output_reg shape is [number of anchors, 18]\n",
    "    Second dimension 0 - 4 are bounding box offset, width and height: dx, dy, w ,h\n",
    "    Second dimension 4 - 18 are 7 hand keypoint x and y coordinates: x1,y1,x2,y2,...x7,y7\n",
    "    \"\"\"\n",
    "    # [2016, 18]\n",
    "    output_reg = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "    \"\"\"\n",
    "    output_clf shape is [number of anchors]\n",
    "    it is the classification score if there is a hand for each anchor box\n",
    "    \"\"\"\n",
    "    # [2016, 1]\n",
    "    output_clf = interpreter.get_tensor(output_details[1]['index'])[0]\n",
    "\n",
    "    return output_reg, output_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(output_reg, output_clf):\n",
    "    def _sigm(x):\n",
    "      return 1 / (1 + np.exp(-x) )\n",
    "    \n",
    "    def _get_triangle(self, kp0, kp2, dist=1):\n",
    "      \"\"\"get a triangle used to calculate Affine transformation matrix\"\"\"\n",
    "\n",
    "      # 90Â° rotation matrix used to create the alignment trianlge\n",
    "      R90 = np.r_[[[0,1],[-1,0]]]\n",
    "\n",
    "      dir_v = kp2 - kp0\n",
    "      dir_v /= np.linalg.norm(dir_v)\n",
    "\n",
    "      dir_v_r = dir_v @ R90.T\n",
    "      return np.float32([kp2, kp2+dir_v*dist, kp2 + dir_v_r*dist])\n",
    "\n",
    "    # Get probabilities\n",
    "    # [2016, 1]\n",
    "    probabilities = _sigm(output_clf)\n",
    "    # [2016]: Boolean\n",
    "    detection_mask = (probabilities > DETECTION_THRESHOLD).flatten()\n",
    "    # [x,18]\n",
    "    candidate_detect = output_reg[detection_mask]\n",
    "    # [x,4]\n",
    "    candidate_anchors = anchors[detection_mask]\n",
    "    # [x, 1]\n",
    "    probabilities = probabilities[detection_mask]\n",
    "\n",
    "    if candidate_detect.shape[0] == 0:\n",
    "      return None, None, None\n",
    "    \n",
    "    # return candidate_detect, candidate_anchors, probabilities\n",
    "\n",
    "    # Pick the best bounding box with non maximum suppression\n",
    "    # the boxes must be moved by the corresponding anchor first\n",
    "    # [x, 18]\n",
    "    moved_candidate_detect = candidate_detect.copy()\n",
    "    moved_candidate_detect[:, :2] = candidate_detect[:, :2] + candidate_anchors[:, :2] * IMG_INPUT_SIZE[0]\n",
    "    box_ids = non_max_suppression_fast(moved_candidate_detect[:, :4], probabilities)\n",
    "\n",
    "    if len(box_ids) == 0:\n",
    "      return None, None, None\n",
    "\n",
    "    return candidate_detect[box_ids][0], candidate_anchors[box_ids][0], probabilities[box_ids][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bbox(candidate_detect, candidate_anchors, shape):\n",
    "  scale_width = shape[0] / IMG_INPUT_SIZE[0]\n",
    "  scale_height = shape[1] / IMG_INPUT_SIZE[1]\n",
    "  bbox = np.zeros((candidate_detect.shape[0], 4))\n",
    "\n",
    "  cx = candidate_detect[:, 0] + candidate_anchors[:, 0] * shape[0]\n",
    "  cy = candidate_detect[:, 1] + candidate_anchors[:, 1] * shape[1]\n",
    "  width = candidate_detect[:, 2] * scale_width\n",
    "  height = candidate_detect[:, 3] * scale_height\n",
    "\n",
    "  bbox[:, 0] = cx - width # x_min\n",
    "  bbox[:, 1] = cy - height # y_min\n",
    "  bbox[:, 2] = cx + width # x_max\n",
    "  bbox[:, 3] =  cy + height # y_max\n",
    "\n",
    "  # Ensure the bounding box coordinates are within the image boundaries\n",
    "  bbox[:, :2] = np.maximum(0, bbox[:, :2])\n",
    "  bbox[:, 2:] = np.minimum(shape[1], bbox[:, 2:])\n",
    "\n",
    "  return bbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.startWindowThread()\n",
    "cv2.namedWindow(WINDOW)\n",
    "capture = cv2.VideoCapture(0)\n",
    "\n",
    "while capture.isOpened():\n",
    "  hasFrame, frame = capture.read()\n",
    "  \n",
    "  if hasFrame:\n",
    "    img_pad, img_norm, pad = preprocess_img(frame)\n",
    "    shape = np.r_[img_pad.shape]\n",
    "    output_reg, output_clf = run_inference(img_norm)\n",
    "    candidate_detect, candidate_anchors, probabilities = process_output(output_reg, output_clf)\n",
    "\n",
    "    if candidate_detect is not None:\n",
    "      bbox = get_bbox(candidate_detect, candidate_anchors, shape)\n",
    "      \n",
    "      # draw box\n",
    "      for box in bbox:\n",
    "        cv2.rectangle(img_pad, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)\n",
    "\n",
    "    cv2.imshow(WINDOW, img_pad)\n",
    "  \n",
    "  if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "    break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Fix for macos bug when closing window\n",
    "for i in range(4):\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
