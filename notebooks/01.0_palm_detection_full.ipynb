{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV: 4.8.1\n",
      "CSV: 1.0\n",
      "Numpy: 1.26.2\n",
      "Pandas: 2.1.1\n",
      "TensorFlow: 2.15.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from cv2.typing import MatLike\n",
    "\n",
    "from non_maximum_suppression import non_max_suppression_fast\n",
    "\n",
    "print(f'OpenCV: {cv2.__version__}')\n",
    "print(f'CSV: {csv.__version__}')\n",
    "print(f'Numpy: {np.__version__}')\n",
    "print(f'Pandas: {pd.__version__}')\n",
    "print(f'TensorFlow: {tf.__version__}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW = 'Hand Tracking'\n",
    "PALM_MODEL_PATH = '../models/palm_detection_full.tflite'\n",
    "ANCHORS_PATH = '../MediaPipeSSDAnchors/anchors.csv'\n",
    "DETECTION_THRESHOLD = 0.5\n",
    "BOX_ENLARGE_FACTOR = 1.5\n",
    "BOX_SHIFT_FACTOR = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anchors shape: (2016, 4)\n",
      "Anchors: [[0.02083333 0.02083333 1.         1.        ]\n",
      " [0.02083333 0.02083333 1.         1.        ]\n",
      " [0.0625     0.02083333 1.         1.        ]\n",
      " [0.0625     0.02083333 1.         1.        ]\n",
      " [0.10416667 0.02083333 1.         1.        ]]\n"
     ]
    }
   ],
   "source": [
    "csv_file = open(ANCHORS_PATH, 'r')\n",
    "anchors = np.r_[\n",
    "  [x for x in csv.reader(csv_file, quoting=csv.QUOTE_NONNUMERIC)]\n",
    "]\n",
    "\n",
    "print(f'Anchors shape: {anchors.shape}')\n",
    "print(f'Anchors: {anchors[:5]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=PALM_MODEL_PATH)\n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                         0\n",
      "name                                                               input_1\n",
      "index                                                                    0\n",
      "shape                                                     [1, 192, 192, 3]\n",
      "shape_signature                                           [1, 192, 192, 3]\n",
      "dtype                                              <class 'numpy.float32'>\n",
      "quantization                                                      (0.0, 0)\n",
      "quantization_parameters  {'scales': [], 'zero_points': [], 'quantized_d...\n",
      "sparsity_parameters                                                     {}\n"
     ]
    }
   ],
   "source": [
    "input_details = interpreter.get_input_details()\n",
    "\n",
    "for detail in input_details:\n",
    "    print(pd.DataFrame.from_dict(detail, orient='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image input size: (192, 192)\n"
     ]
    }
   ],
   "source": [
    "IMG_INPUT_SIZE = tuple(input_details[0]['shape'][1:3])\n",
    "\n",
    "print(f'Image input size: {IMG_INPUT_SIZE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                         0\n",
      "name                                                              Identity\n",
      "index                                                                  279\n",
      "shape                                                        [1, 2016, 18]\n",
      "shape_signature                                              [1, 2016, 18]\n",
      "dtype                                              <class 'numpy.float32'>\n",
      "quantization                                                      (0.0, 0)\n",
      "quantization_parameters  {'scales': [], 'zero_points': [], 'quantized_d...\n",
      "sparsity_parameters                                                     {}\n",
      "                                                                         0\n",
      "name                                                            Identity_1\n",
      "index                                                                  276\n",
      "shape                                                         [1, 2016, 1]\n",
      "shape_signature                                               [1, 2016, 1]\n",
      "dtype                                              <class 'numpy.float32'>\n",
      "quantization                                                      (0.0, 0)\n",
      "quantization_parameters  {'scales': [], 'zero_points': [], 'quantized_d...\n",
      "sparsity_parameters                                                     {}\n"
     ]
    }
   ],
   "source": [
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "for detail in output_details:\n",
    "    print(pd.DataFrame.from_dict(detail, orient='index'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_img(frame: MatLike):\n",
    "    # Original Size\n",
    "    shape = np.r_[frame.shape]\n",
    "\n",
    "    # Calculate Padding\n",
    "    pad = (shape.max() - shape[:2]).astype('uint32') // 2\n",
    "\n",
    "    # Pad Image\n",
    "    img_pad = np.pad(frame, ((pad[0], pad[0]), (pad[1], pad[1]), (0, 0)), 'constant')\n",
    "\n",
    "    # Resize Image\n",
    "    img_small = cv2.resize(img_pad, IMG_INPUT_SIZE)\n",
    "    \n",
    "    # Set as contiguous array (speed up)\n",
    "    img_small = np.ascontiguousarray(img_small)\n",
    "\n",
    "    img_norm = (img_small / 255).astype('float32')\n",
    "    \n",
    "    return img_pad, img_norm, pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(img_norm):\n",
    "    # Add Batch dimension\n",
    "    img_input = np.expand_dims(img_norm, axis=0)\n",
    "\n",
    "    # Set input tensor\n",
    "    interpreter.set_tensor(input_details[0]['index'], img_input)\n",
    "\n",
    "    # Run Inference\n",
    "    interpreter.invoke()\n",
    "\n",
    "    # Get Output\n",
    "    \"\"\"\n",
    "    output_reg shape is [number of anchors, 18]\n",
    "    Second dimension 0 - 4 are bounding box offset, width and height: dx, dy, w ,h\n",
    "    Second dimension 4 - 18 are 7 hand keypoint x and y coordinates: x1,y1,x2,y2,...x7,y7\n",
    "    \"\"\"\n",
    "    output_reg = interpreter.get_tensor(output_details[0]['index'])[0]\n",
    "\n",
    "    \"\"\"\n",
    "    output_clf shape is [number of anchors]\n",
    "    it is the classification score if there is a hand for each anchor box\n",
    "    \"\"\"\n",
    "    output_clf = interpreter.get_tensor(output_details[1]['index'])[0]\n",
    "\n",
    "    return output_reg, output_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_output(output_reg, output_clf):\n",
    "    def _sigm(x):\n",
    "      return 1 / (1 + np.exp(-x) )\n",
    "    \n",
    "    def _get_triangle(self, kp0, kp2, dist=1):\n",
    "      \"\"\"get a triangle used to calculate Affine transformation matrix\"\"\"\n",
    "\n",
    "      # 90Â° rotation matrix used to create the alignment trianlge\n",
    "      R90 = np.r_[[[0,1],[-1,0]]]\n",
    "\n",
    "      dir_v = kp2 - kp0\n",
    "      dir_v /= np.linalg.norm(dir_v)\n",
    "\n",
    "      dir_v_r = dir_v @ R90.T\n",
    "      return np.float32([kp2, kp2+dir_v*dist, kp2 + dir_v_r*dist])\n",
    "\n",
    "    # Get probabilities\n",
    "    probabilities = _sigm(output_clf)\n",
    "    detection_mask = (probabilities > DETECTION_THRESHOLD).flatten()\n",
    "    candidate_detect = output_reg[detection_mask]\n",
    "    candidate_anchors = anchors[detection_mask]\n",
    "    probabilities = probabilities[detection_mask]\n",
    "\n",
    "    if candidate_detect.shape[0] == 0:\n",
    "      return None, None, None\n",
    "\n",
    "    # Pick the best bounding box with non maximum suppression\n",
    "    # the boxes must be moved by the corresponding anchor first\n",
    "    moved_candidate_detect = candidate_detect.copy()\n",
    "    moved_candidate_detect[:, :2] = candidate_detect[:, :2] + (candidate_anchors[:, :2] * IMG_INPUT_SIZE[0])\n",
    "    box_ids = non_max_suppression_fast(moved_candidate_detect[:, :4], probabilities)\n",
    "\n",
    "    # Pick the first detected hand. Could be adapted for multi hand recognition\n",
    "    box_ids = box_ids[0]\n",
    "\n",
    "    # bounding box offsets, width and height\n",
    "    dx,dy,w,h = candidate_detect[box_ids, :4][0]\n",
    "    # center_wo_offst = candidate_anchors[box_ids,:2] * IMG_INPUT_SIZE[0]\n",
    "\n",
    "    # # 7 initial keypoints\n",
    "    # keypoints = center_wo_offst + candidate_detect[box_ids,4:].reshape(-1,2)\n",
    "    # side = max(w,h) * BOX_ENLARGE_FACTOR\n",
    "\n",
    "    # # now we need to move and rotate the detected hand for it to occupy a\n",
    "    # # IMG_INPUT_SIZE square\n",
    "    # # line from wrist keypoint to middle finger keypoint\n",
    "    # # should point straight up\n",
    "    # # TODO: replace triangle with the bbox directly\n",
    "    # source = _get_triangle(keypoints[0], keypoints[2], side)\n",
    "    # source -= (keypoints[0] - keypoints[2]) * BOX_SHIFT_FACTOR\n",
    "\n",
    "    # debug_info = {\n",
    "    #   \"detection_candidates\": candidate_detect,\n",
    "    #   \"anchor_candidates\": candidate_anchors,\n",
    "    #   \"selected_box_id\": box_ids,\n",
    "    # }\n",
    "\n",
    "    # return source, keypoints, debug_info\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-04 23:27:30.499 python[24809:1315475] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.7048548 -3.7318597 21.25844   21.258087 ]\n",
      "1.7048548\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m   shape \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mr_[img_pad\u001b[39m.\u001b[39mshape]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m   output_reg, output_clf \u001b[39m=\u001b[39m run_inference(img_norm)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m   source, keypoints, debug_info \u001b[39m=\u001b[39m process_output(output_reg, output_clf)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m   cv2\u001b[39m.\u001b[39mimshow(WINDOW, img_pad)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mif\u001b[39;00m cv2\u001b[39m.\u001b[39mwaitKey(\u001b[39m10\u001b[39m) \u001b[39m&\u001b[39m \u001b[39m0xFF\u001b[39m \u001b[39m==\u001b[39m \u001b[39mord\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq\u001b[39m\u001b[39m'\u001b[39m):\n",
      "\u001b[1;32m/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb Cell 11\u001b[0m line \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=43'>44</a>\u001b[0m side \u001b[39m=\u001b[39m \u001b[39mmax\u001b[39m(w,h) \u001b[39m*\u001b[39m BOX_ENLARGE_FACTOR\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m# now we need to move and rotate the detected hand for it to occupy a\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m# IMG_INPUT_SIZE square\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m# line from wrist keypoint to middle finger keypoint\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m# should point straight up\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m# TODO: replace triangle with the bbox directly\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m source \u001b[39m=\u001b[39m _get_triangle(keypoints[\u001b[39m0\u001b[39m], keypoints[\u001b[39m2\u001b[39m], side)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m source \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m (keypoints[\u001b[39m0\u001b[39m] \u001b[39m-\u001b[39m keypoints[\u001b[39m2\u001b[39m]) \u001b[39m*\u001b[39m BOX_SHIFT_FACTOR\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m debug_info \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mdetection_candidates\u001b[39m\u001b[39m\"\u001b[39m: candidate_detect,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39manchor_candidates\u001b[39m\u001b[39m\"\u001b[39m: candidate_anchors,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mselected_box_id\u001b[39m\u001b[39m\"\u001b[39m: box_ids,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m }\n",
      "\u001b[1;32m/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m dir_v \u001b[39m/\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mnorm(dir_v)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m dir_v_r \u001b[39m=\u001b[39m dir_v \u001b[39m@\u001b[39m R90\u001b[39m.\u001b[39mT\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/luisciber/Development/hand_sign_detection/notebooks/01.0_palm_detection_full.ipynb#W5sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mfloat32([kp2, kp2\u001b[39m+\u001b[39mdir_v\u001b[39m*\u001b[39mdist, kp2 \u001b[39m+\u001b[39m dir_v_r\u001b[39m*\u001b[39mdist])\n",
      "\u001b[0;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (3,) + inhomogeneous part."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "cv2.startWindowThread()\n",
    "cv2.namedWindow(WINDOW)\n",
    "capture = cv2.VideoCapture(1)\n",
    "\n",
    "while capture.isOpened():\n",
    "  hasFrame, frame = capture.read()\n",
    "  \n",
    "  if hasFrame:\n",
    "    img_pad, img_norm, pad = preprocess_img(frame)\n",
    "    shape = np.r_[img_pad.shape]\n",
    "    output_reg, output_clf = run_inference(img_norm)\n",
    "    source, keypoints, debug_info = process_output(output_reg, output_clf)\n",
    "\n",
    "    cv2.imshow(WINDOW, img_pad)\n",
    "  \n",
    "  if cv2.waitKey(10) & 0xFF == ord('q'):\n",
    "    break\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Fix for macos bug when closing window\n",
    "for i in range(4):\n",
    "    cv2.waitKey(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
